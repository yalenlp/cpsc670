- date: Tues 01/17/23
  lecturer: Arman
  title:
    - Course introduction
    - Logistics
    - Transformers
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_1.pdf
  readings:
    - Attention is all you need (2017) <a href="https://arxiv.org/abs/1706.03762" target="_blank">[link]</a>
  logistics:

- date: Thu 01/19/23
  lecturer: Arman
  title:
    - Transfer learning
    - Pre-training
    - Pre-trained transformers
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_2.pdf
  # slides2:
  logistics:
  readings:
    - ELMo, Deep Contextualized Representations <a href="https://arxiv.org/pdf/1802.05365.pdf">[link]</a>
    - ULMFit, Universal Language Model Fine-tuning for Text Classification <a href="https://arxiv.org/pdf/1801.06146.pdf">[link]</a>
    - BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/pdf/1810.04805.pdf">[link]</a>
  optional:
    - <p> RoBERTa, A Robustly Optimized BERT Pretraining Approach <a href="https://arxiv.org/pdf/1907.11692.pdf">[link]</a> </p>

- date: Tue 01/24/23
  lecturer: Kejian; Huangrui
  question-form: https://forms.gle/EBNHeejzviX5pKvz5
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_3_gpt2.pdf
  slides2: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_3_t5.pdf
  title:
    - Transfer learning
    - Pre-training
  readings:
    - Language Models are Unsupervised Multitask Learners (GPT-2) <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf " target="_blank">[link]</a>
    - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) <a href="https://arxiv.org/pdf/1910.10683.pdf">[link]</a>
  optional:
    - BART, Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <a href="https://arxiv.org/pdf/1910.13461.pdf">[link]</a>
    - ELECTRA, Pre-training Text Encoders as Discriminators Rather Than Generators <a href="https://arxiv.org/abs/2003.10555">[link]</a>

  logistics:

- date: Th 01/26/23
  lecturer: Zhangir
  title: Model Architecture and Training Objectives
  slides: /cpsc670/assets/lectures/s23/lecture_4.pdf
  question-form: https://forms.gle/wqjDZ8cj3Buuu6yt8
  readings:
    - UL2- Unifying Language Learning Paradigms (2022) <a href="https://arxiv.org/pdf/2205.05131.pdf">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (2022) <a href="https://arxiv.org/pdf/2204.05832.pdf">[link]</a>
  logistics:

- date: Tue 01/31/23
  lecturer: Ruixiao, Chenxi
  question-form: https://forms.gle/xGXqs38oTsYjVkFu8
  title:
    - Scaling Laws for Langauge Models
    - Tips on choosing a project [Arman]
  notes:
  readings:
    - Scaling Laws for Neural Language Models (2020) <a href="https://arxiv.org/pdf/2001.08361.pdf " target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models (2022) <a href="https://arxiv.org/pdf/2203.15556.pdf" target="blank">[link]</a>
  logistics: <a href="/cpsc670/assets/lectures/s23/lecture_5_projects.pdf"> [Tips on choosing projects]</a> <br/><br/> <a href="https://docs.google.com/document/d/1JBCkDdlywQcQtxz6fHg1p2ev0GgLbmHbcAg4NRIu-Mc/edit">[Projects Doc]</a>
  optional:
    - Scale Efficiently - Insights from Pre-training and Fine-tuning Transformers (2021) <a href="https://arxiv.org/pdf/2109.10686.pdf">[link]</a>

- date: Th 02/02/23
  lecturer: Veer; Ziqing
  question-form: https://forms.gle/XUmGv8DyzTcab7Fq5
  title:
    - LLMs
    - Power of scale
  readings:
    - Language Models are Few-Shot Learners (GPT3; 2020) <a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank">[link]</a>
    - PaLM - Scaling Language Modeling with Pathways (2022) <a href="https://arxiv.org/pdf/2204.02311.pdf">[link]</a>

- date: Tue 2/7/23
  lecturer: Ayla, Hailey
  title:
    - Prompting and Few-shot learning
  readings:
    - Itâ€™s Not Just Size That Matters- Small Language Models Are Also Few-Shot Learners <a href="https://arxiv.org/pdf/2009.07118.pdf" traget="_blank">[link]</a>
    - Making Pre-trained Language Models Better Few-shot Learners <a href="https://arxiv.org/pdf/2012.15723.pdf" traget="_blank">[link]</a>

- date: Thu 2/9/23
  lecturer: Yujie; David
  title:
    - Prompting and ICL
    - Why ICL works
  readings:
    - Rethinking the Role of Demonstrations- What Makes In-Context Learning Work? <a href="https://arxiv.org/pdf/2202.12837.pdf" traget="_blank">[link]</a>
    - Data Distributional Properties Drive Emergent In-Context Learning in Transformers <a href="https://arxiv.org/pdf/2205.05055.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">Finalize project teams</span>

- date: Tue 2/14/23
  lecturer:
  title:
    - Why ICL works
    - Instruction tuning
  readings:
    - Impact of Pretraining Term Frequencies on Few-Shot Reasoning <a href="https://arxiv.org/pdf/2202.07206.pdf" traget="_blank">[link]</a>
    - Training language models to follow instructions with human feedback <a href="https://arxiv.org/pdf/2203.02155.pdf" traget="_blank">[link]</a>

- date: Thu 2/16/23
  lecturer:
  title:
    - Instruction tuning
    - Parameter efficient fine-tuning
  readings:
    - Scaling Instruction-Finetuned Language Models <a href="https://arxiv.org/pdf/2210.11416.pdf" traget="_blank">[link]</a>
    - Parameter-Efficient Transfer Learning for NLP <a href="https://arxiv.org/pdf/1902.00751.pdf" traget="_blank">[link]</a>

- date: Tues 2/21/23
  lecturer:
  title:
    - Parameter efficient fine-tuning
  readings:
    - Prefix-Tuning- Optimizing Continuous Prompts for Generation <a href="https://arxiv.org/pdf/2101.00190.pdf" traget="_blank">[link]</a>
    - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning <a href="https://arxiv.org/pdf/2205.05638.pdf" traget="_blank">[link]</a>

- date: Thu 2/23/23
  lecturer:
  title:
    - Chain of thought reasoning
    - Emergent properties
  readings:
    - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <a href="https://arxiv.org/pdf/2201.11903.pdf" traget="_blank">[link]</a>
    - Emergent Abilities of Large Language Models <a href="https://openreview.net/pdf?id=yzkSU5zdwD" traget="_blank">[link]</a>

- date: Tue 2/28/23
  lecturer:
  title:
    - Efficient transformers
  readings:
    - Longformer- The Long-Document Transformer <a href="https://arxiv.org/pdf/2004.05150.pdf" traget="_blank">[link]</a>
    - Big Bird- Transformers for Longer Sequences <a href="https://arxiv.org/pdf/2007.14062.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">Project proposals due</span>

- date: Thu 3/2/23
  lecturer:
  title:
    - Efficient transformers
  readings:
    - On the Parameterization and Initialization of Diagonal State Space Models <a href="https://arxiv.org/pdf/2206.11893.pdf" traget="_blank">[link]</a>
    - (this paper is dense and we will only discuss 1 paper in this session. The paper will be presented by 2 students <a href=")" traget="_blank">[link]</a>

- date: Tue 3/7/23
  lecturer:
  title:
    - Memory
  readings:
    - Memorizing Transformers <a href="https://arxiv.org/pdf/2203.08913.pdf" traget="_blank">[link]</a>
    - Training Language Models with Memory Augmentation <a href="https://arxiv.org/pdf/2205.12674.pdf" traget="_blank">[link]</a>

- date: Thu 3/9/23
  lecturer:
  title:
    - Retrieval and retrieval-augmented models
  readings:
    - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <a href="https://arxiv.org/pdf/2005.11401.pdf" traget="_blank">[link]</a>
    - Improving language models by retrieving from trillions of tokens <a href="https://arxiv.org/pdf/2112.04426.pdf" traget="_blank">[link]</a>

- date: 3/14 - 3/26
  lecturer:
  title: >
    <strong> Spring break - No classes </strong>
  recitation:

- date: Tue 3/28/23
  lecturer:
  title:
    - Decoding approaches
  readings:
    - NEUROLOGIC A*esque Decoding- Constrained Text Generation with Lookahead Heuristics <a href="https://arxiv.org/pdf/2112.08726.pdf" traget="_blank">[link]</a>
    - Generating Sequences by Learning to Self-Correct <a href="https://arxiv.org/pdf/2211.00053.pdf" traget="_blank">[link]</a>

- date: Thu 3/30/23
  lecturer:
  title:
    - Societal and ethical considerations, bias, safety in AI/NLP
  readings:
    - On the Dangers of Stochastic Parrots- Can Language Models Be Too Big? <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922" traget="_blank">[link]</a>
    - Self-Diagnosis and Self-Debiasing- A Proposal for Reducing Corpus-Based Bias in NLP <a href="https://arxiv.org/pdf/2103.00453.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">4/1 Progress report due</span>

- date: 4/4/23
  lecturer:
  title:
    - Mixture of experts, Hypernetworks,
  readings:
    - Switch Transformers- Scaling to Trillion Parameter Models <a href="https://arxiv.org/pdf/2101.03961.pdf" traget="_blank">[link]</a>
    - Hyperdecoders- Instance-specific decoders for multi-task NLP <a href="https://arxiv.org/pdf/2203.08304.pdf" traget="_blank">[link]</a>

- date: 4/6/23
  lecturer:
  title:
    - Data
    - Spurious biases
    - Dataset difficulty
  readings:
    - Competency Problems- On Finding and Removing Artifacts in Language Data <a href="https://arxiv.org/pdf/2104.08646.pdf" traget="_blank">[link]</a>
    - Understanding Dataset Difficulty with V-Usable Information <a href="https://arxiv.org/pdf/2110.08420.pdf" traget="_blank">[link]</a>

- date: 4/11/23
  lecturer:
  title:
    - Multi-modal models
  readings:
    - CM3- A Causal Masked Multimodal Model of the Internet <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>
    - Flamingo- a Visual Language Model for Few-Shot Learning <a href="https://arxiv.org/pdf/2204.14198.pdf" traget="_blank">[link]</a>

- date: 4/13/23
  lecturer:
  title:
    - Retrieval and retrieval-augmented models
  readings:
    - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <a href="https://arxiv.org/pdf/2005.11401.pdf" traget="_blank">[link]</a>
    - Improving language models by retrieving from trillions of tokens <a href="https://arxiv.org/pdf/2112.04426.pdf" traget="_blank">[link]</a>

- date: 4/18/23
  lecturer:
  title:
    - AI Alignment
  readings:
    - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>
    - A General Language Assistant as a Laboratory for Alignment <a href="https://arxiv.org/pdf/2112.00861.pdf"" traget="_blank">[link]</a>
    - Fine-tuning language models to find agreement among humans with diverse preferences <a href="https://arxiv.org/pdf/2211.15006.pdf" traget="_blank">[link]</a>

- date: 4/20/23
  lecturer:
  title:
    - LLMs for Code
  readings:
    - CodeGen- An Open Large Language Model for Code with Multi-Turn Program Synthesis <a href="https://arxiv.org/pdf/2203.13474.pdf" traget="_blank">[link]</a>
    - InCoder- A Generative Model for Code Infilling and Synthesis <a href="https://arxiv.org/pdf/2204.05999.pdf" traget="_blank">[link]</a>

- date: 4/25/23
  lecturer:
  title:
    - Efficiency
  readings:
    - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
    - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>

- date: 4/27/23
  lecturer: All students
  title:
    - Final presentations for projects
  # readings:
  #   - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
  #   - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">4/27 Presentations; 5/10 Final project report</span>
