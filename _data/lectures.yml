- date: Tues 01/17/23
  lecturer:
    - Arman
  title:
    - Course introduction
    - Logistics
    - Transformers
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_1.pdf
  readings:
    - Attention is all you need (2017) <a href="https://arxiv.org/abs/1706.03762" target="_blank">[link]</a>
  logistics:

- date: Thu 01/19/23
  lecturer:
    - Arman
  title:
    - Transfer learning
    - Pre-training
    - Pre-trained transformers
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_2.pdf
  # slides2:
  logistics:
  readings:
    - ELMo, Deep Contextualized Representations <a href="https://arxiv.org/pdf/1802.05365.pdf">[link]</a>
    - ULMFit, Universal Language Model Fine-tuning for Text Classification <a href="https://arxiv.org/pdf/1801.06146.pdf">[link]</a>
    - BERT, Pre-training of Deep Bidirectional Transformers for Language Understanding <a href="https://arxiv.org/pdf/1810.04805.pdf">[link]</a>
  optional:
    - <p> RoBERTa, A Robustly Optimized BERT Pretraining Approach <a href="https://arxiv.org/pdf/1907.11692.pdf">[link]</a> </p>

- date: Tue 01/24/23
  lecturer:
    - Kejian
    - Huangrui
  question-form: https://forms.gle/EBNHeejzviX5pKvz5
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_3_gpt2.pdf
  slides2: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_3_t5.pdf
  title:
    - Transfer learning
    - Pre-training
  readings:
    - Language Models are Unsupervised Multitask Learners (GPT-2) <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf " target="_blank">[link]</a>
    - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) <a href="https://arxiv.org/pdf/1910.10683.pdf">[link]</a>
  optional:
    - BART, Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension <a href="https://arxiv.org/pdf/1910.13461.pdf">[link]</a>
    - ELECTRA, Pre-training Text Encoders as Discriminators Rather Than Generators <a href="https://arxiv.org/abs/2003.10555">[link]</a>

  logistics:

- date: Th 01/26/23
  lecturer:
    - Zhangir
  title: Model Architecture and Training Objectives
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_4_ul2.pdf
  question-form: https://forms.gle/wqjDZ8cj3Buuu6yt8
  readings:
    - UL2- Unifying Language Learning Paradigms (2022) <a href="https://arxiv.org/pdf/2205.05131.pdf">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (2022) <a href="https://arxiv.org/pdf/2204.05832.pdf">[link]</a>
  logistics:

- date: Tue 01/31/23
  lecturer:
    - Ruixiao
    - Chenxi
  question-form: https://forms.gle/xGXqs38oTsYjVkFu8
  title:
    - Scaling Laws for Langauge Models
    - Tips on choosing a project [Arman]
  notes:
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_5_scaling_laws.pdf
  slides2: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_5_chinchilla.pdf
  readings:
    - Scaling Laws for Neural Language Models (2020) <a href="https://arxiv.org/pdf/2001.08361.pdf " target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models (2022) <a href="https://arxiv.org/pdf/2203.15556.pdf" target="blank">[link]</a>
  logistics: <a href="https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_5_projects.pdf"> [Tips on choosing projects]</a> <br/><br/> <a href="https://docs.google.com/document/d/1JBCkDdlywQcQtxz6fHg1p2ev0GgLbmHbcAg4NRIu-Mc/edit">[Projects Doc]</a>
  optional:
    - Scale Efficiently - Insights from Pre-training and Fine-tuning Transformers (2021) <a href="https://arxiv.org/pdf/2109.10686.pdf">[link]</a>

- date: Th 02/02/23
  lecturer:
    - Arman
    - Ziqing
  question-form: https://forms.gle/XUmGv8DyzTcab7Fq5
  title:
    - LLMs
    - Power of scale
  readings:
    - Language Models are Few-Shot Learners (GPT3; 2020) <a href="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank">[link]</a>
    - PaLM - Scaling Language Modeling with Pathways (2022) <a href="https://arxiv.org/pdf/2204.02311.pdf">[link]</a>
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_6_gpt3.pdf
  slides2: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_6_palm.pdf

- date: Tue 2/7/23
  lecturer:
    - Ayla
    - Hailey
  title:
    - Prompting and Few-shot learning
  readings:
    - Itâ€™s Not Just Size That Matters- Small Language Models Are Also Few-Shot Learners (2020) <a href="https://arxiv.org/pdf/2009.07118.pdf" traget="_blank">[link]</a>
    - Making Pre-trained Language Models Better Few-shot Learners (2021) <a href="https://arxiv.org/pdf/2012.15723.pdf" traget="_blank">[link]</a>
  question-form: https://forms.gle/tPn1iGLN6Cp88TXo7
  slides: https://yalenlp.github.io/cpsc670/assets/lectures/s23/lecture_7_pet.pdf
  slides2: https://docs.google.com/presentation/d/1_VGuTUS-JvmOTRwoXNPLUA3ykkBfJWU12UG27zUMGls/edit

- date: Thu 2/9/23
  lecturer:
    - Yujie
    - David
  title:
    - Prompting and ICL
    - Why ICL works
  readings:
    - Rethinking the Role of Demonstrations- What Makes In-Context Learning Work? (2022) <a href="https://arxiv.org/pdf/2202.12837.pdf" traget="_blank">[link]</a>
    - Data Distributional Properties Drive Emergent In-Context Learning in Transformers (2022) <a href="https://arxiv.org/pdf/2205.05055.pdf" traget="_blank">[link]</a>
  optional:
    - What learning algorithm is in-context learning? Investigations with linear models (2022) <a href="https://arxiv.org/pdf/2211.15661.pdf" target="_blank">[link]</a>
    - Transformers as Algorithms- Generalization and Stability in In-context Learning (2023) <a href="https://arxiv.org/pdf/2301.07067.pdf" target="_blank">[link]</a>
  logistics:
    - <span class="deadline">Finalize project teams</span>
  question-form: https://forms.gle/R39R6FoKEK55NrQP9

- date: Tue 2/14/23
  lecturer:
    - Huangrui
    - Hyoungseob
  title:
    - Instruction tuning
  readings:
    - Training language models to follow instructions with human feedback (2022) <a href="https://arxiv.org/pdf/2203.02155.pdf" traget="_blank">[link]</a>
    - Scaling Instruction-Finetuned Language Models (2022) <a href="https://arxiv.org/pdf/2210.11416.pdf" traget="_blank">[link]</a>
  question-form: https://forms.gle/pSixeeAx6GKCsCGBA

- date: Thu 2/16/23
  lecturer:
    - Zhangir
    - Hailey
  title:
    - Parameter efficient fine-tuning
  readings:
    - Towards a Unified View of Parameter-Efficient Transfer Learning (2021) <a href="https://arxiv.org/pdf/2110.04366.pdf" traget="_blank">[link]</a>
    - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning (2022) <a href="https://arxiv.org/pdf/2205.05638.pdf" traget="_blank">[link]</a>
  question-form: https://forms.gle/wwAbTF5YmzUSTHv56

  optional:
    - Parameter-Efficient Transfer Learning for NLP (2019) <a href="https://arxiv.org/pdf/1902.00751.pdf" traget="_blank">[link]</a>
    - Prefix-Tuning- Optimizing Continuous Prompts for Generation (2021) <a href="https://arxiv.org/pdf/2101.00190.pdf" traget="_blank">[link]</a>

- date: Tues 2/21/23
  lecturer:
    - Ayla
    - Ziqing
  title:
    - Chain of thought reasoning
    - Emergence
  question-form: https://forms.gle/J2RECiWE7SBn4H6J8
  readings:
    - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (2022) <a href="https://arxiv.org/pdf/2201.11903.pdf" traget="_blank">[link]</a>
    - Emergent Abilities of Large Language Models (2022) <a href="https://openreview.net/pdf?id=yzkSU5zdwD" traget="_blank">[link]</a>

- date: Thu 2/23/23
  lecturer:
    - Kejian
    - Ruixiao
  title:
    - Efficient transformers
  readings:
    - Longformer- The Long-Document Transformer (2020) <a href="https://arxiv.org/pdf/2004.05150.pdf" traget="_blank">[link]</a>
    - Big Bird- Transformers for Longer Sequences (2020) <a href="https://arxiv.org/pdf/2007.14062.pdf" traget="_blank">[link]</a>
  question-form: https://forms.gle/f3rMudVhJk6UQF3U9
  optional:
    - Efficient Transformers- A Survey <a href="https://arxiv.org/pdf/2009.06732.pdf" target="_blank">[link] </a>

- date: Tue 2/28/23
  lecturer:
    - David
    - Yujie
  title:
    - Efficient transformers
  readings:
    - Efficiently Modeling Long Sequences with Structured State Spaces (2021) <a href="https://arxiv.org/pdf/2111.00396.pdf" traget="_blank">[link]</a>
    - Simplifying S4 <a href="https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4" target="_blank">[link]</a>
  optional:
    - On the Parameterization and Initialization of Diagonal State Space Models (2022) <a href="https://arxiv.org/pdf/2206.11893.pdf" traget="_blank">[link]</a>
    - Hungry Hungry Hippos- Towards Language Modeling with State Space Models (2022) <a href="https://arxiv.org/pdf/2212.14052.pdf" traget="_blank">[link]</a>
    - FlashAttention- Fast and Memory-Efficient Exact Attention with IO-Awareness (2022) <a href="https://arxiv.org/pdf/2205.14135.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">Project proposals due</span>

- date: Thu 3/2/23
  lecturer: Hailey
  title:
    - Memory
  readings:
    - Memorizing Transformers (2022) <a href="https://arxiv.org/pdf/2203.08913.pdf" traget="_blank">[link]</a>
  optional:
    - Training Language Models with Memory Augmentation (2022) <a href="https://arxiv.org/pdf/2205.12674.pdf" traget="_blank">[link]</a>

- date: Tue 3/7/23
  lecturer:
  title:
    - <b> Guest Lecture</b>
    - <a href="https://scottyih.org/">Scott Yih</a> - Efficient and Scalable NLP through Retrieval-Augmented Language Models
  readings:
    - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (2020) <a href="https://arxiv.org/pdf/2005.11401.pdf" traget="_blank">[link]</a>
    - Improving language models by retrieving from trillions of tokens (2021) <a href="https://arxiv.org/pdf/2112.04426.pdf" traget="_blank">[link]</a>
    - REPLUG, Retrieval-Augmented Black-Box Language Models <a href="https://arxiv.org/pdf/2301.12652.pdf" target="_blank">[link]</a>
    - Retrieval-Augmented Multimodal Language Modeling <a href="https://arxiv.org/pdf/2211.12561.pdf" target="_blank">[link]</a>

- date: Thu 3/9/23
  lecturer: Ziqing
  title:
    - Iterative methods
  readings:
    - PEER- A Collaborative Language Model (2022) <a href="https://arxiv.org/pdf/2208.11663.pdf" traget="_blank">[link]</a>
  optional:
    - Generating Sequences by Learning to Self-Correct (2022) <a href="https://arxiv.org/pdf/2211.00053.pdf" traget="_blank">[link]</a>

- date: 3/14 - 3/26
  lecturer:
  title: >
    <strong> Spring break - No classes </strong>
  recitation:

- date: Tue 3/28/23
  lecturer:
  title:
    - <b> Guest Lecture</b>
    - <a href="https://wellecks.com/">Sean Welleck</a> - Decoding methods for natural language generation
  readings:
    - NEUROLOGIC DECODING- (Un)supervised Neural Text Generation with Predicate Logic Constraints (2021) <a href="https://aclanthology.org/2021.naacl-main.339.pdf" target="_blank">[link]</a>
    - NEUROLOGIC A*esque Decoding- Constrained Text Generation with Lookahead Heuristics (2022) <a href="https://arxiv.org/pdf/2112.08726.pdf" traget="_blank">[link]</a>

- date: Thu 3/30/23
  lecturer: Yujie
  title:
    - Societal and ethical considerations, bias, safety in AI/NLP
  readings:
    - On the Dangers of Stochastic Parrots- Can Language Models Be Too Big? (2020) <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922" traget="_blank">[link]</a>
  optional:
    - Self-Diagnosis and Self-Debiasing- A Proposal for Reducing Corpus-Based Bias in NLP (2021) <a href="https://arxiv.org/pdf/2103.00453.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">4/1 Progress report due</span>

- date: 4/4/23
  lecturer: Huangrui
  title:
    - Mixture of experts and sparse models
  readings:
    - Switch Transformers- Scaling to Trillion Parameter Models (2021) <a href="https://arxiv.org/pdf/2101.03961.pdf" traget="_blank">[link]</a>
    # - Hyperdecoders- Instance-specific decoders for multi-task NLP (2022) <a href="https://arxiv.org/pdf/2203.08304.pdf" traget="_blank">[link]</a>

- date: 4/6/23
  lecturer: Ruixiao
  title:
    - Data
    - Spurious biases
    - Dataset difficulty
  readings:
    - Competency Problems- On Finding and Removing Artifacts in Language Data (2021) <a href="https://arxiv.org/pdf/2104.08646.pdf" traget="_blank">[link]</a>
  optional:
    - Understanding Dataset Difficulty with V-Usable Information (2021) <a href="https://arxiv.org/pdf/2110.08420.pdf" traget="_blank">[link]</a>

- date: 4/11/23
  lecturer: Ayla
  title:
    - Multi-modal models
  readings:
    # - CM3- A Causal Masked Multimodal Model of the Internet (2022) <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>
    - Flamingo- a Visual Language Model for Few-Shot Learning (2022) <a href="https://arxiv.org/pdf/2204.14198.pdf" traget="_blank">[link]</a>
  optional:
    - CM3- A Causal Masked Multimodal Model of the Internet (2022) <a href="https://arxiv.org/pdf/2201.07520.pdf" traget="_blank">[link]</a>

- date: 4/13/23
  lecturer: Zhangir
  title:
    - Training LLMs with human feedback
    - AI Alignment
  readings:
    # - Learning to Summarize with Human Feedback (2020) <a href="https://arxiv.org/pdf/2009.01325.pdf">[link]</a>
    - A General Language Assistant as a Laboratory for Alignment (2021) <a href="https://arxiv.org/pdf/2112.00861.pdf" traget="_blank">[link]</a>
  optional:
    - Learning to Summarize with Human Feedback (2020) <a href="https://arxiv.org/pdf/2009.01325.pdf">[link]</a>


- date: 4/18/23
  lecturer: David
  title:
    - AI Alignment
  readings:
    # - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022) <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>
    - Fine-tuning language models to find agreement among humans with diverse preferences (2022) <a href="https://arxiv.org/pdf/2211.15006.pdf" traget="_blank">[link]</a>
  optional: 
    - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022) <a href="https://arxiv.org/pdf/2204.05862.pdf" traget="_blank">[link]</a>

- date: 4/20/23
  lecturer: Kejian
  title:
    - LLMs for Code
  readings:
    - CodeGen- An Open Large Language Model for Code with Multi-Turn Program Synthesis (2022) <a href="https://arxiv.org/pdf/2203.13474.pdf" traget="_blank">[link]</a>
    - InCoder- A Generative Model for Code Infilling and Synthesis (2022) <a href="https://arxiv.org/pdf/2204.05999.pdf" traget="_blank">[link]</a>

- date: 4/25/23
  lecturer:
  title:
    - <b> Guest Lecture</b>
    - <a href="https://timdettmers.com/about/">Tim Dettmers</a> - Making LLMs more efficient
  readings:
    - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
    - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>

- date: 4/27/23
  lecturer: All students
  title:
    - Final presentations for projects
  # readings:
  #   - LLM.int8()- 8-bit Matrix Multiplication for Transformers at Scale <a href="https://arxiv.org/pdf/2208.07339.pdf" traget="_blank">[link]</a>
  #   - Efficiently Scaling Transformer Inference <a href="https://arxiv.org/pdf/2211.05102.pdf" traget="_blank">[link]</a>
  logistics:
    - <span class="deadline">4/27 Presentations; 5/10 Final project report</span>
